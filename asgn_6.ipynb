{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Q1 Create a dataset (.csv file) having following features- experience of the candidate,\n",
        "written score, interview score and salary. Based on three input features, HR decide\n",
        "the salary of the selected candidates. Using this data, KNN model build a for HR\n",
        "department that can help them decide salaries of the candidates. Predict the salaries\n",
        "for the following candidates, by executing the model (for different values of K):\n",
        "\n",
        "(a) 5 Yrs experience, 8 written test score, 10 interview score\n",
        "\n",
        "(b) 8 Yrs experience, 7 written test score, 6 interview score"
      ],
      "metadata": {
        "id": "R0UB-jKlxhiV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.neighbors import KNeighborsRegressor"
      ],
      "metadata": {
        "id": "q2ipFOAxuNR2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = {\n",
        "    'Experience': ['5','3','8','6','4','10','2','1','12','24','8','6','4','10','2','1','5','3','8','6','4','10','2','8','6','4','10','2','5','15'],\n",
        "    'Written Score': ['20','40','40','25','50','30','20','45','46','48','30','28','29','40','50','34','35','38','48','36','44','50','42','28','46','44','31','34','30','30'],\n",
        "    'Interview Score': ['8','5','8','8','4','9','6','4','10','10','7','5','3','5','7','8','4','6','2','2','3','4','5','3','5','7','8','4','5','10'],\n",
        "    'Salary': ['60000','45000','75000','65000','50000','80000','55000','48000','95000','100000','75000','70000','62000','90000','105000','72000','65000','85000','54000','60000','65000','74000','98000','75000','77000','86000','74000','69000','61000','130000']\n",
        "}"
      ],
      "metadata": {
        "id": "KslR2jpVurZt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.DataFrame(data)\n"
      ],
      "metadata": {
        "id": "ief4k82-ux69"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert string columns to numeric\n",
        "df['Experience'] = df['Experience'].astype(int)\n",
        "df['Written Score'] = df['Written Score'].astype(int)\n",
        "df['Interview Score'] = df['Interview Score'].astype(int)\n",
        "df['Salary'] = df['Salary'].astype(int)"
      ],
      "metadata": {
        "id": "1gOihIoYuz0P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x=df[['Experience', 'Written Score', 'Interview Score']]\n",
        "y=df[['Salary']]"
      ],
      "metadata": {
        "id": "ECq9syyku4sL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the data into training and testing sets\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "KaHZP1f3vPxI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Standardize the features\n",
        "scaler = StandardScaler()\n",
        "x_train_scaled = scaler.fit_transform(x_train)\n",
        "x_test_scaled = scaler.transform(x_test)"
      ],
      "metadata": {
        "id": "NU2myU5evTjt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "knn_model = KNeighborsRegressor(n_neighbors=3)  # You can adjust the number of neighbors (K)\n",
        "knn_model.fit(x_train_scaled, y_train)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 75
        },
        "id": "NqLZg_VOvn48",
        "outputId": "638bfc2c-082f-4ff7-ec44-4c5e44235958"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "KNeighborsRegressor(n_neighbors=3)"
            ],
            "text/html": [
              "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>KNeighborsRegressor(n_neighbors=3)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">KNeighborsRegressor</label><div class=\"sk-toggleable__content\"><pre>KNeighborsRegressor(n_neighbors=3)</pre></div></div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = knn_model.predict(x_test_scaled)"
      ],
      "metadata": {
        "id": "T9aCxuiUvyCP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yV8nX0J6v2gB",
        "outputId": "d6582862-2586-4d92-fb9a-d81c6d9dde30"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[57333.33333333],\n",
              "       [76333.33333333],\n",
              "       [64000.        ],\n",
              "       [76333.33333333],\n",
              "       [95000.        ],\n",
              "       [94666.66666667]])"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "r2 = r2_score(y_test, y_pred)"
      ],
      "metadata": {
        "id": "pRYykpRpv8gc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Mean Squared Error: {mse}\")\n",
        "print(f\"R^2 Score: {r2}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5dTN2iv9wAHl",
        "outputId": "c3dc8fae-e06a-4a71-b2a5-da1e2972743a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Squared Error: 63240740.74074074\n",
            "R^2 Score: 0.5357531266992932\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "candidate_features = [\n",
        "    [5, 8, 10],  # Candidate (a): 5 Yrs experience, 8 written test score, 10 interview score\n",
        "    [8, 7, 6]    # Candidate (b): 8 Yrs experience, 7 written test score, 6 interview score\n",
        "]\n",
        "\n"
      ],
      "metadata": {
        "id": "R1df3qa9wMi8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Standardize the candidate features using the previously fitted scaler\n",
        "candidate_features_scaled = scaler.transform(candidate_features)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q8zz-xHywO_E",
        "outputId": "a9e03b3f-4735-4a94-f8d9-349a6ee10350"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Predict salaries for the candidates\n",
        "candidate_salaries = knn_model.predict(candidate_features_scaled)"
      ],
      "metadata": {
        "id": "OHKl4CrEwTjx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the predicted salaries for the candidates\n",
        "for i, salary in enumerate(candidate_salaries):\n",
        "    print(f'Predicted salary for candidate {chr(97 + i)}: {salary:.2f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 176
        },
        "id": "g3hRXvlYwV5y",
        "outputId": "cc4455e1-adc4-456e-c98a-802f1d643c08"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "unsupported format string passed to numpy.ndarray.__format__",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-32-81f7ae1a43b8>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Print the predicted salaries for the candidates\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msalary\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcandidate_salaries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Predicted salary for candidate {chr(97 + i)}: {salary:.2f}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m: unsupported format string passed to numpy.ndarray.__format__"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#gemini\n",
        "import pandas as pd\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Sample data (replace with your actual data)\n",
        "data = {\n",
        "    'Experience': [2, 4, 6, 1, 3, 5, 7, 8],\n",
        "    'Written Score': [7, 9, 8, 5, 6, 10, 8, 7],\n",
        "    'Interview Score': [8, 7, 10, 6, 8, 9, 6, 5],\n",
        "    'Salary': [40000, 55000, 70000, 35000, 50000, 80000, 65000, 52000]\n",
        "}\n",
        "\n",
        "# Create DataFrame and save as CSV\n",
        "df = pd.DataFrame(data)\n",
        "df.to_csv('salary_data.csv', index=False)\n",
        "\n",
        "# Load data from CSV\n",
        "df = pd.read_csv('salary_data.csv')\n",
        "\n",
        "# Features and target variable\n",
        "X = df[['Experience', 'Written Score', 'Interview Score']]\n",
        "y = df['Salary']\n",
        "\n",
        "# Train-Test Split (adjust test_size as needed)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Standardize the features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Define K values to try\n",
        "k_values = [3, 5, 4]\n",
        "\n",
        "# Function to predict salary for a candidate\n",
        "def predict_salary(experience, written_score, interview_score, knn_model):\n",
        "  candidate_data = [[experience, written_score, interview_score]]\n",
        "  predicted_salary = knn_model.predict(candidate_data)[0]\n",
        "  return predicted_salary\n",
        "\n",
        "# Train and predict for different K values\n",
        "for k in k_values:\n",
        "  # Create and train KNN model\n",
        "  knn_model = KNeighborsRegressor(n_neighbors=k)\n",
        "  knn_model.fit(X_train, y_train)\n",
        "\n",
        "  # Candidate (a)\n",
        "  predicted_salary_a = predict_salary(5, 8, 10, knn_model)\n",
        "  print(f\"Candidate (a) - Predicted Salary (K={k}): ${predicted_salary_a:.2f}\")\n",
        "\n",
        "  # Candidate (b)\n",
        "  predicted_salary_b = predict_salary(8, 7, 6, knn_model)\n",
        "  print(f\"Candidate (b) - Predicted Salary (K={k}): ${predicted_salary_b:.2f}\")\n",
        "  print(\"-\" * 40)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6JmtuRnY0hft",
        "outputId": "92e69df9-2ab2-46b2-bdcb-1ff08e3cc8ba"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Candidate (a) - Predicted Salary (K=3): $53333.33\n",
            "Candidate (b) - Predicted Salary (K=3): $62333.33\n",
            "----------------------------------------\n",
            "Candidate (a) - Predicted Salary (K=5): $55400.00\n",
            "Candidate (b) - Predicted Salary (K=5): $55400.00\n",
            "----------------------------------------\n",
            "Candidate (a) - Predicted Salary (K=4): $56250.00\n",
            "Candidate (b) - Predicted Salary (K=4): $59250.00\n",
            "----------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but KNeighborsRegressor was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but KNeighborsRegressor was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but KNeighborsRegressor was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but KNeighborsRegressor was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but KNeighborsRegressor was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but KNeighborsRegressor was fitted with feature names\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "this was Q1"
      ],
      "metadata": {
        "id": "q4KhL3HtxdGz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q3 For the IRIS dataset, design a decision tree classifier. Take different percentage of\n",
        "training data and then observe effect on the accuracy and other quality parameters.\n",
        "Also note the effect of other decision tree parameters (like max depth,\n",
        "min_sample_spit etc.) on the performance of the model.\n",
        "Note: Take criterion as entropy."
      ],
      "metadata": {
        "id": "D7mid1jSzM_p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import GaussianNB"
      ],
      "metadata": {
        "id": "b6L7rjRmwYxs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X, y = load_iris(return_X_y=True)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.6, random_state=0)\n",
        "gnb = GaussianNB()\n",
        "y_pred = gnb.fit(X_train, y_train).predict(X_test)\n",
        "print(\"Number of mislabeled points out of a total points :\", ((y_test != y_pred).sum(), X_test.shape[0]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rQNH9REIzRji",
        "outputId": "64a63ce3-5aa3-403e-b531-fd5f5dbd4ecc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of mislabeled points out of a total points : (5, 90)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#acc to gemini\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data  # Features\n",
        "y = iris.target  # Target labels\n",
        "\n",
        "# Define evaluation metrics function\n",
        "def evaluate_model(model, X_train, X_test, y_train, y_test):\n",
        "  model.fit(X_train, y_train)\n",
        "  y_pred = model.predict(X_test)\n",
        "  accuracy = accuracy_score(y_test, y_pred)\n",
        "  precision = precision_score(y_test, y_pred, average='weighted')\n",
        "  recall = recall_score(y_test, y_pred, average='weighted')\n",
        "  f1 = f1_score(y_test, y_pred, average='weighted')\n",
        "  return accuracy, precision, recall, f1\n",
        "\n",
        "# Define training data percentages to try\n",
        "train_percentages = [0.6, 0.7, 0.8, 0.9]\n",
        "\n",
        "# Define decision tree parameters to explore\n",
        "max_depths = [3, 5, None]  # None for maximum depth\n",
        "min_samples_splits = [2, 5, 10]\n",
        "\n",
        "# Iterate through training data percentages\n",
        "for train_perc in train_percentages:\n",
        "  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=1 - train_perc, random_state=42)\n",
        "\n",
        "  # Iterate through max depths\n",
        "  for max_depth in max_depths:\n",
        "    # Iterate through min samples per split\n",
        "    for min_samples_split in min_samples_splits:\n",
        "      # Create and train the decision tree model\n",
        "      model = DecisionTreeClassifier(criterion='entropy', max_depth=max_depth, min_samples_split=min_samples_split)\n",
        "      accuracy, precision, recall, f1 = evaluate_model(model, X_train, X_test, y_train, y_test)\n",
        "\n",
        "      # Print results\n",
        "      print(f\"Training Data: {train_perc*100:.1f}%, Max Depth: {max_depth}, Min Samples Split: {min_samples_split}\")\n",
        "      print(f\"Accuracy: {accuracy:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}, F1-Score: {f1:.4f}\")\n",
        "      print(\"-\" * 80)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4sMFecDm0KdD",
        "outputId": "ccdb1d62-7e7b-4271-8a27-de24b1c3f5fa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Data: 60.0%, Max Depth: 3, Min Samples Split: 2\n",
            "Accuracy: 0.9833, Precision: 0.9842, Recall: 0.9833, F1-Score: 0.9833\n",
            "--------------------------------------------------------------------------------\n",
            "Training Data: 60.0%, Max Depth: 3, Min Samples Split: 5\n",
            "Accuracy: 0.9833, Precision: 0.9842, Recall: 0.9833, F1-Score: 0.9833\n",
            "--------------------------------------------------------------------------------\n",
            "Training Data: 60.0%, Max Depth: 3, Min Samples Split: 10\n",
            "Accuracy: 0.9833, Precision: 0.9842, Recall: 0.9833, F1-Score: 0.9833\n",
            "--------------------------------------------------------------------------------\n",
            "Training Data: 60.0%, Max Depth: 5, Min Samples Split: 2\n",
            "Accuracy: 0.9667, Precision: 0.9667, Recall: 0.9667, F1-Score: 0.9667\n",
            "--------------------------------------------------------------------------------\n",
            "Training Data: 60.0%, Max Depth: 5, Min Samples Split: 5\n",
            "Accuracy: 0.9667, Precision: 0.9667, Recall: 0.9667, F1-Score: 0.9667\n",
            "--------------------------------------------------------------------------------\n",
            "Training Data: 60.0%, Max Depth: 5, Min Samples Split: 10\n",
            "Accuracy: 0.9833, Precision: 0.9842, Recall: 0.9833, F1-Score: 0.9833\n",
            "--------------------------------------------------------------------------------\n",
            "Training Data: 60.0%, Max Depth: None, Min Samples Split: 2\n",
            "Accuracy: 0.9833, Precision: 0.9842, Recall: 0.9833, F1-Score: 0.9833\n",
            "--------------------------------------------------------------------------------\n",
            "Training Data: 60.0%, Max Depth: None, Min Samples Split: 5\n",
            "Accuracy: 0.9667, Precision: 0.9667, Recall: 0.9667, F1-Score: 0.9667\n",
            "--------------------------------------------------------------------------------\n",
            "Training Data: 60.0%, Max Depth: None, Min Samples Split: 10\n",
            "Accuracy: 0.9833, Precision: 0.9842, Recall: 0.9833, F1-Score: 0.9833\n",
            "--------------------------------------------------------------------------------\n",
            "Training Data: 70.0%, Max Depth: 3, Min Samples Split: 2\n",
            "Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1-Score: 1.0000\n",
            "--------------------------------------------------------------------------------\n",
            "Training Data: 70.0%, Max Depth: 3, Min Samples Split: 5\n",
            "Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1-Score: 1.0000\n",
            "--------------------------------------------------------------------------------\n",
            "Training Data: 70.0%, Max Depth: 3, Min Samples Split: 10\n",
            "Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1-Score: 1.0000\n",
            "--------------------------------------------------------------------------------\n",
            "Training Data: 70.0%, Max Depth: 5, Min Samples Split: 2\n",
            "Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1-Score: 1.0000\n",
            "--------------------------------------------------------------------------------\n",
            "Training Data: 70.0%, Max Depth: 5, Min Samples Split: 5\n",
            "Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1-Score: 1.0000\n",
            "--------------------------------------------------------------------------------\n",
            "Training Data: 70.0%, Max Depth: 5, Min Samples Split: 10\n",
            "Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1-Score: 1.0000\n",
            "--------------------------------------------------------------------------------\n",
            "Training Data: 70.0%, Max Depth: None, Min Samples Split: 2\n",
            "Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1-Score: 1.0000\n",
            "--------------------------------------------------------------------------------\n",
            "Training Data: 70.0%, Max Depth: None, Min Samples Split: 5\n",
            "Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1-Score: 1.0000\n",
            "--------------------------------------------------------------------------------\n",
            "Training Data: 70.0%, Max Depth: None, Min Samples Split: 10\n",
            "Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1-Score: 1.0000\n",
            "--------------------------------------------------------------------------------\n",
            "Training Data: 80.0%, Max Depth: 3, Min Samples Split: 2\n",
            "Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1-Score: 1.0000\n",
            "--------------------------------------------------------------------------------\n",
            "Training Data: 80.0%, Max Depth: 3, Min Samples Split: 5\n",
            "Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1-Score: 1.0000\n",
            "--------------------------------------------------------------------------------\n",
            "Training Data: 80.0%, Max Depth: 3, Min Samples Split: 10\n",
            "Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1-Score: 1.0000\n",
            "--------------------------------------------------------------------------------\n",
            "Training Data: 80.0%, Max Depth: 5, Min Samples Split: 2\n",
            "Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1-Score: 1.0000\n",
            "--------------------------------------------------------------------------------\n",
            "Training Data: 80.0%, Max Depth: 5, Min Samples Split: 5\n",
            "Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1-Score: 1.0000\n",
            "--------------------------------------------------------------------------------\n",
            "Training Data: 80.0%, Max Depth: 5, Min Samples Split: 10\n",
            "Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1-Score: 1.0000\n",
            "--------------------------------------------------------------------------------\n",
            "Training Data: 80.0%, Max Depth: None, Min Samples Split: 2\n",
            "Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1-Score: 1.0000\n",
            "--------------------------------------------------------------------------------\n",
            "Training Data: 80.0%, Max Depth: None, Min Samples Split: 5\n",
            "Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1-Score: 1.0000\n",
            "--------------------------------------------------------------------------------\n",
            "Training Data: 80.0%, Max Depth: None, Min Samples Split: 10\n",
            "Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1-Score: 1.0000\n",
            "--------------------------------------------------------------------------------\n",
            "Training Data: 90.0%, Max Depth: 3, Min Samples Split: 2\n",
            "Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1-Score: 1.0000\n",
            "--------------------------------------------------------------------------------\n",
            "Training Data: 90.0%, Max Depth: 3, Min Samples Split: 5\n",
            "Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1-Score: 1.0000\n",
            "--------------------------------------------------------------------------------\n",
            "Training Data: 90.0%, Max Depth: 3, Min Samples Split: 10\n",
            "Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1-Score: 1.0000\n",
            "--------------------------------------------------------------------------------\n",
            "Training Data: 90.0%, Max Depth: 5, Min Samples Split: 2\n",
            "Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1-Score: 1.0000\n",
            "--------------------------------------------------------------------------------\n",
            "Training Data: 90.0%, Max Depth: 5, Min Samples Split: 5\n",
            "Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1-Score: 1.0000\n",
            "--------------------------------------------------------------------------------\n",
            "Training Data: 90.0%, Max Depth: 5, Min Samples Split: 10\n",
            "Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1-Score: 1.0000\n",
            "--------------------------------------------------------------------------------\n",
            "Training Data: 90.0%, Max Depth: None, Min Samples Split: 2\n",
            "Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1-Score: 1.0000\n",
            "--------------------------------------------------------------------------------\n",
            "Training Data: 90.0%, Max Depth: None, Min Samples Split: 5\n",
            "Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1-Score: 1.0000\n",
            "--------------------------------------------------------------------------------\n",
            "Training Data: 90.0%, Max Depth: None, Min Samples Split: 10\n",
            "Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1-Score: 1.0000\n",
            "--------------------------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "this was Q3"
      ],
      "metadata": {
        "id": "thRAVyS-zVZr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q2\n",
        "Create a dataset (.csv file) having following features- Graduations percentage,\n",
        "experience of the candidate, written score, interview score and selection. Selection\n",
        "feature is binary in nature and contains the status of the candidate. Also store at least\n",
        "25 records in this dataset.\n",
        "Using this data, build a Bayesian learning model for HR department that can help\n",
        "them to decide whether the candidate will be selected or not. Take 80% data as\n",
        "training data and remaining a testing data randomly. Using the built model, predict\n",
        "the status for the following unseen data:\n",
        "\n",
        "(a) 90 %, 5 Yrs experience, 8 written test score, 10 interview score\n",
        "\n",
        "(b) 75%, 8 Yrs experience, 7 written test score, 6 interview score\n",
        "Also calculate the possible classification metrics for the above cases and save these\n",
        "values in the .CSV file."
      ],
      "metadata": {
        "id": "_9Vqm6_eBV2a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "# Sample candidate data (replace with your actual data)\n",
        "candidates = [\n",
        "    {'Graduation Percentage': 85, 'Experience': 2, 'Written Score': 7, 'Interview Score': 8, 'Selection': 1},\n",
        "    {'Graduation Percentage': 90, 'Experience': 4, 'Written Score': 9, 'Interview Score': 10, 'Selection': 1},\n",
        "    # Add at least 23 more records here following the same format\n",
        "    {'Graduation Percentage': 75, 'Experience': 8, 'Written Score': 7, 'Interview Score': 6, 'Selection': 0},  # Candidate (b) - Add this for unseen data prediction\n",
        "    {'Graduation Percentage': 90, 'Experience': 5, 'Written Score': 8, 'Interview Score': 10, 'Selection': 1},  # Candidate (a) - Add this for unseen data prediction\n",
        "]\n",
        "\n",
        "# Create DataFrame and save as CSV\n",
        "df = pd.DataFrame(candidates)\n",
        "df.to_csv('candidate_data.csv', index=False)  # Save as CSV\n",
        "\n",
        "# Load data from CSV\n",
        "df = pd.read_csv('candidate_data.csv')\n",
        "\n",
        "# Features and target variable\n",
        "X = df[['Graduation Percentage', 'Experience', 'Written Score', 'Interview Score']]\n",
        "y = df['Selection']\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Build Naive Bayes model\n",
        "model = MultinomialNB()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict selection status for unseen candidates\n",
        "candidate_a = [90, 5, 8, 10]  # Graduation Percentage, Experience, Written Score, Interview Score (Candidate a)\n",
        "candidate_b = [75, 8, 7, 6]  # Graduation Percentage, Experience, Written Score, Interview Score (Candidate b)\n",
        "\n",
        "predicted_a = model.predict([candidate_a])[0]\n",
        "predicted_b = model.predict([candidate_b])[0]\n",
        "\n",
        "# Calculate classification metrics for testing data\n",
        "accuracy = accuracy_score(y_test, model.predict(X_test))\n",
        "precision = precision_score(y_test, model.predict(X_test), average='weighted')\n",
        "recall = recall_score(y_test, model.predict(X_test), average='weighted')\n",
        "f1 = f1_score(y_test, model.predict(X_test), average='weighted')\n",
        "\n",
        "# Prepare results for unseen candidates and metrics\n",
        "results = {\n",
        "    'Candidate (a)': {\n",
        "        'Predicted Selection': predicted_a,\n",
        "        'Actual Selection (from data)': candidate_a[-1]  # Last element of candidate_a is selection status\n",
        "    },\n",
        "    'Candidate (b)': {\n",
        "        'Predicted Selection': predicted_b,\n",
        "        'Actual Selection (from data)': candidate_b[-1]  # Last element of candidate_b is selection status\n",
        "    },\n",
        "    'Testing Data Performance': {\n",
        "        'Accuracy': accuracy,\n",
        "        'Precision': precision,\n",
        "        'Recall': recall,\n",
        "        'F1-Score': f1\n",
        "    }\n",
        "}\n",
        "\n",
        "print(\"**Unseen Candidate Predictions:**\")\n",
        "for candidate, prediction in results.items():\n",
        "  if candidate.startswith('Candidate'):  # Print only candidate results\n",
        "    print(f\"\\t{candidate}:\")\n",
        "    print(f\"\\t\\tPredicted Selection: {prediction['Predicted Selection']}\")\n",
        "    print(f\"\\t\\tActual Selection (from data): {prediction['Actual Selection (from data)']}\")\n",
        "\n",
        "print(\"\\n**Testing Data Performance:**\")\n",
        "for metric, value in results['Testing Data Performance'].items():\n",
        "  print(f\"\\t{metric}: {value:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ICVdihXhzTpR",
        "outputId": "8a3ae02b-1d47-4539-a907-14c6f3be46a9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "**Unseen Candidate Predictions:**\n",
            "\tCandidate (a):\n",
            "\t\tPredicted Selection: 1\n",
            "\t\tActual Selection (from data): 10\n",
            "\tCandidate (b):\n",
            "\t\tPredicted Selection: 0\n",
            "\t\tActual Selection (from data): 6\n",
            "\n",
            "**Testing Data Performance:**\n",
            "\tAccuracy: 1.0000\n",
            "\tPrecision: 1.0000\n",
            "\tRecall: 1.0000\n",
            "\tF1-Score: 1.0000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MultinomialNB was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MultinomialNB was fitted with feature names\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q4 By taking Classified Data as input, compare the performance of KNN, Bayesian\n",
        "Classifier and Decision Tree model. In this comparison, you can take different\n",
        "possible parameters of particular model."
      ],
      "metadata": {
        "id": "-_SwMAGrEux1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.naive_bayes import GaussianNB, MultinomialNB\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "def compare_models(data_file, target_column, knn_params, nb_types, dt_params):\n",
        "  \"\"\"\n",
        "  Compares performance of KNN, Naive Bayes, and Decision Tree models\n",
        "  on a given classified dataset.\n",
        "\n",
        "  Args:\n",
        "      data_file (str): Path to the CSV file containing classified data.\n",
        "      target_column (str): Name of the column containing the target variable.\n",
        "      knn_params (list): List of parameter dictionaries for KNN models.\n",
        "      nb_types (list): List of Naive Bayes classifier types (e.g., [\"GaussianNB\", \"MultinomialNB\"]).\n",
        "      dt_params (list): List of parameter dictionaries for Decision Tree models.\n",
        "  \"\"\"\n",
        "\n",
        "  # Load data\n",
        "  data = {\n",
        "    'Experience': ['5','3','8','6','4','10','2','1','12','24','8','6','4','10','2','1','5','3','8','6','4','10','2','8','6','4','10','2','5','15'],\n",
        "    'Written Score': ['20','40','40','25','50','30','20','45','46','48','30','28','29','40','50','34','35','38','48','36','44','50','42','28','46','44','31','34','30','30'],\n",
        "    'Interview Score': ['8','5','8','8','4','9','6','4','10','10','7','5','3','5','7','8','4','6','2','2','3','4','5','3','5','7','8','4','5','10'],\n",
        "    'Salary': ['60000','45000','75000','65000','50000','80000','55000','48000','95000','100000','75000','70000','62000','90000','105000','72000','65000','85000','54000','60000','65000','74000','98000','75000','77000','86000','74000','69000','61000','130000']\n",
        "  }\n",
        "  df = pd.DataFrame(data)\n",
        "  df['Experience'] = df['Experience'].astype(int)\n",
        "  df['Written Score'] = df['Written Score'].astype(int)\n",
        "  df['Interview Score'] = df['Interview Score'].astype(int)\n",
        "  df['Salary'] = df['Salary'].astype(int)\n",
        "  X=df[['Experience', 'Written Score', 'Interview Score']]\n",
        "  y=df[['Salary']]\n",
        "\n",
        "  # Split data into training and testing sets\n",
        "  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "  # Define results list\n",
        "  results = []\n",
        "\n",
        "  # KNN models\n",
        "  for param_set in knn_params:\n",
        "    knn_model = KNeighborsClassifier(**param_set)\n",
        "    knn_model.fit(X_train, y_train)\n",
        "    y_pred = knn_model.predict(X_test)\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    results.append({\"Model\": \"KNN\", \"Parameters\": param_set, \"Accuracy\": accuracy})\n",
        "\n",
        "  # Naive Bayes models\n",
        "  for nb_type in nb_types:\n",
        "    nb_model = eval(nb_type)()  # Dynamically create model instance\n",
        "    nb_model.fit(X_train, y_train)\n",
        "    y_pred = nb_model.predict(X_test)\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    results.append({\"Model\": nb_type, \"Parameters\": \"NA\", \"Accuracy\": accuracy})\n",
        "\n",
        "  # Decision Tree models\n",
        "  for param_set in dt_params:\n",
        "    dt_model = DecisionTreeClassifier(**param_set)\n",
        "    dt_model.fit(X_train, y_train)\n",
        "    y_pred = dt_model.predict(X_test)\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    results.append({\"Model\": \"Decision Tree\", \"Parameters\": param_set, \"Accuracy\": accuracy})\n",
        "\n",
        "  # Print results\n",
        "  for result in results:\n",
        "    print(f\"Model: {result['Model']}\")\n",
        "    print(f\"Parameters: {result['Parameters']}\")\n",
        "    print(f\"Accuracy: {result['Accuracy']:.4f}\")\n",
        "    print(\"-\" * 30)\n",
        "\n",
        "# Example usage (replace with your data and parameter details)\n",
        "data_file = \"your_data.csv\"\n",
        "target_column = \"selection\"  # Replace with your target column name\n",
        "knn_params = [{\"n_neighbors\": [3, 5, 7]}, {\"weights\": [\"uniform\", \"distance\"]}]\n",
        "nb_types = [\"GaussianNB\", \"MultinomialNB\"]\n",
        "dt_params = [{\"max_depth\": [3, 5, None]}, {\"min_samples_split\": [2, 5, 10]}]\n",
        "\n",
        "compare_models(data_file, target_column, knn_params, nb_types, dt_params)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        },
        "id": "q5BBg2a5Ex77",
        "outputId": "7e31b5d1-d172-4d68-d418-ad046e983fdf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "InvalidParameterError",
          "evalue": "The 'n_neighbors' parameter of KNeighborsClassifier must be an int in the range [1, inf) or None. Got [3, 5, 7] instead.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mInvalidParameterError\u001b[0m                     Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-52-2d669f1fbb65>\u001b[0m in \u001b[0;36m<cell line: 80>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[0mdt_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"max_depth\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"min_samples_split\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m \u001b[0mcompare_models\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_column\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mknn_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnb_types\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdt_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-52-2d669f1fbb65>\u001b[0m in \u001b[0;36mcompare_models\u001b[0;34m(data_file, target_column, knn_params, nb_types, dt_params)\u001b[0m\n\u001b[1;32m     43\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mparam_set\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mknn_params\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0mknn_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKNeighborsClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mparam_set\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m     \u001b[0mknn_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m     \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mknn_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0maccuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/neighbors/_classification.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    211\u001b[0m             \u001b[0mThe\u001b[0m \u001b[0mfitted\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mnearest\u001b[0m \u001b[0mneighbors\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m         \"\"\"\n\u001b[0;32m--> 213\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    214\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36m_validate_params\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    598\u001b[0m         \u001b[0maccepted\u001b[0m \u001b[0mconstraints\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    599\u001b[0m         \"\"\"\n\u001b[0;32m--> 600\u001b[0;31m         validate_parameter_constraints(\n\u001b[0m\u001b[1;32m    601\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parameter_constraints\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    602\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdeep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/_param_validation.py\u001b[0m in \u001b[0;36mvalidate_parameter_constraints\u001b[0;34m(parameter_constraints, params, caller_name)\u001b[0m\n\u001b[1;32m     95\u001b[0m                 )\n\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m             raise InvalidParameterError(\n\u001b[0m\u001b[1;32m     98\u001b[0m                 \u001b[0;34mf\"The {param_name!r} parameter of {caller_name} must be\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m                 \u001b[0;34mf\" {constraints_str}. Got {param_val!r} instead.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mInvalidParameterError\u001b[0m: The 'n_neighbors' parameter of KNeighborsClassifier must be an int in the range [1, inf) or None. Got [3, 5, 7] instead."
          ]
        }
      ]
    }
  ]
}